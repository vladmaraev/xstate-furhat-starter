# Report on lab 3

In this experiment, I used Furhat *remote api* and *web interface* to work with *main.ts*. It is thrilling to adjust its `Mask`, `Voice`, `Character` and `Gestures`. Additionally, I involved a custom sound effect to make its response more human-like. 

The obstacle for me is to create new gestures -- it is too subtle to control, and the given parameters lack descriptions. I intially wanted to create a *nodGesture*, but I didn't find a parameter to make the neck move up and down. Maybe this difficulty is what makes social robots so fascinating, sometimes micro facial expressions or body gestures tell more than verbal expressions. 

Another limitation is that I have not yet known how to call LLMs in Furhat. I just hard-coded a simple dialogue this time. 

In the future, I want to try calling LLMs in it, and creating different characters to adapt to different scenarios (maybe by using different role setting and prompts?), as showcased in demo videos.